var documenterSearchIndex = {"docs":
[{"location":"api/#ParallelUtilities.jl","page":"Reference","title":"ParallelUtilities.jl","text":"","category":"section"},{"location":"api/","page":"Reference","title":"Reference","text":"Modules = [ParallelUtilities]","category":"page"},{"location":"api/#ParallelUtilities.AbstractConstrainedProduct","page":"Reference","title":"ParallelUtilities.AbstractConstrainedProduct","text":"AbstractConstrainedProduct{T, N, Q}\n\nSupertype of ProductSplit and ProductSection.\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelUtilities.BroadcastFunction","page":"Reference","title":"ParallelUtilities.BroadcastFunction","text":"BroadcastFunction(f)\n\nConstruct a binary function that evaluates f.(x, y) given the arguments x and y.\n\nnote: Note\nThe function BroadcastFunction(f) is equivalent to Base.BroadcastFunction(f) on Julia versions 1.6 and above.\n\nExamples\n\njulia> ParallelUtilities.BroadcastFunction(+)(ones(3), ones(3))\n3-element Vector{Float64}:\n 2.0\n 2.0\n 2.0\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelUtilities.BroadcastStack","page":"Reference","title":"ParallelUtilities.BroadcastStack","text":"BroadcastStack(f, dims)(x::AbstractArray, y::AbstractArray)\n\nConstruct a binary function that stacks its arguments along dims, with overlapping indices I being replaced by f(x[I], y[I]). The arguments x and y must both be n-dimensional arrays that have identical axes along all dimensions aside from those specified by dims. The axes of the result along each dimensions d in dims would be union(axes(x, d), axes(y, d)). Along the other dimensions the result has the same axes as x and y.\n\nnote: Note\nIf the resulting axes along the concatenated dimensions are not 1-based, one might require an offset array package such as OffsetArrays.jl.\n\nExamples\n\njulia> A = ones(2)*2\n2-element Vector{Float64}:\n 2.0\n 2.0\n\njulia> B = ones(3)*3\n3-element Vector{Float64}:\n 3.0\n 3.0\n 3.0\n\njulia> ParallelUtilities.BroadcastStack(min, 1)(A, B)\n3-element Vector{Float64}:\n 2.0\n 2.0\n 3.0\n\njulia> A = ones(2,2)*2\n2×2 Matrix{Float64}:\n 2.0  2.0\n 2.0  2.0\n\njulia> B = ones(2,3)*3\n2×3 Matrix{Float64}:\n 3.0  3.0  3.0\n 3.0  3.0  3.0\n\njulia> ParallelUtilities.BroadcastStack(+, 2)(A, B)\n2×3 Matrix{Float64}:\n 5.0  5.0  3.0\n 5.0  5.0  3.0\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelUtilities.Commutative","page":"Reference","title":"ParallelUtilities.Commutative","text":"Commutative\n\nDeclare a reduction operator to be commutative in its arguments. No check is performed to ascertain if the operator is indeed commutative.\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelUtilities.Flip","page":"Reference","title":"ParallelUtilities.Flip","text":"Flip(f)\n\nFlip the arguments of a binary function f, so that Flip(f)(x, y) == f(y,x).\n\nExamples\n\njulia> flip1 = ParallelUtilities.Flip(vcat);\n\njulia> flip1(2, 3)\n2-element Vector{Int64}:\n 3\n 2\n\nTwo flips pop the original function back:\n\njulia> flip2 = ParallelUtilities.Flip(flip1);\n\njulia> flip2(2, 3)\n2-element Vector{Int64}:\n 2\n 3\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelUtilities.ProductSection","page":"Reference","title":"ParallelUtilities.ProductSection","text":"ProductSection{T, N, Q<:NTuple{N,AbstractRange}}\n\nIterator that loops over a specified section of the outer product of ranges in. If the ranges are strictly increasing, the iteration will be in reverse - lexicographic order. Given N ranges, each element returned by the iterator will be a tuple of length N with one element from each range.\n\nSee also: ProductSplit\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelUtilities.ProductSection-Tuple{Tuple{Vararg{AbstractRange, N} where N}, AbstractUnitRange}","page":"Reference","title":"ParallelUtilities.ProductSection","text":"ProductSection(iterators::Tuple{Vararg{AbstractRange}}, inds::AbstractUnitRange)\n\nConstruct a ProductSection iterator that represents a 1D view of the outer product of the ranges provided in iterators, with the range of indices in the view being specified by inds.\n\nExamples\n\njulia> p = ParallelUtilities.ProductSection((1:3, 4:6), 5:8);\n\njulia> collect(p)\n4-element Vector{Tuple{Int64, Int64}}:\n (2, 5)\n (3, 5)\n (1, 6)\n (2, 6)\n\njulia> collect(p) == collect(Iterators.product(1:3, 4:6))[5:8]\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.ProductSplit","page":"Reference","title":"ParallelUtilities.ProductSplit","text":"ProductSplit{T, N, Q<:NTuple{N,AbstractRange}}\n\nIterator that loops over a section of the outer product of ranges. If the ranges are strictly increasing, the iteration is in reverse - lexicographic order. Given N ranges, each element returned by the iterator will be a tuple of length N with one element from each range.\n\nSee also: ProductSection\n\n\n\n\n\n","category":"type"},{"location":"api/#ParallelUtilities.ProductSplit-Tuple{Tuple{Vararg{AbstractRange, N} where N}, Integer, Integer}","page":"Reference","title":"ParallelUtilities.ProductSplit","text":"ProductSplit(iterators::Tuple{Vararg{AbstractRange}}, np::Integer, p::Integer)\n\nConstruct a ProductSplit iterator that represents the outer product of the iterators split over np workers, with this instance reprsenting the values on the p-th worker.\n\nnote: Note\np here refers to the rank of the worker, and is unrelated to the worker ID obtained by executing myid() on that worker.\n\nExamples\n\njulia> ParallelUtilities.ProductSplit((1:2, 4:5), 2, 1) |> collect\n2-element Vector{Tuple{Int64, Int64}}:\n (1, 4)\n (2, 4)\n\njulia> ParallelUtilities.ProductSplit((1:2, 4:5), 2, 2) |> collect\n2-element Vector{Tuple{Int64, Int64}}:\n (1, 5)\n (2, 5)\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.broadcastinplace-Union{Tuple{N}, Tuple{Any, Val{N}}} where N","page":"Reference","title":"ParallelUtilities.broadcastinplace","text":"broadcastinplace(f, ::Val{N}) where {N}\n\nConstruct a binary operator that evaluates f.(x, y) and overwrites the Nth argument with the result. For N == 1 this evaluates x .= f.(x, y), whereas for N == 2 this evaluates y .= f.(x, y).\n\nExamples\n\njulia> op = ParallelUtilities.broadcastinplace(+, Val(1));\n\njulia> x = ones(3); y = ones(3);\n\njulia> op(x, y)\n3-element Vector{Float64}:\n 2.0\n 2.0\n 2.0\n\njulia> x # overwritten\n3-element Vector{Float64}:\n 2.0\n 2.0\n 2.0\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.childindex-Tuple{ParallelUtilities.AbstractConstrainedProduct, Any}","page":"Reference","title":"ParallelUtilities.childindex","text":"childindex(ps::AbstractConstrainedProduct, ind)\n\nReturn a tuple containing the indices of the individual AbstractRanges corresponding to the element that is present at index ind in the outer product of the ranges.\n\nnote: Note\nThe index ind corresponds to the outer product of the ranges, and not to ps.\n\nExamples\n\njulia> iters = (1:5, 2:4, 1:3);\n\njulia> ps = ParallelUtilities.ProductSplit(iters, 7, 1);\n\njulia> ind = 6;\n\njulia> cinds = ParallelUtilities.childindex(ps, ind)\n(1, 2, 1)\n\njulia> v = collect(Iterators.product(iters...));\n\njulia> getindex.(iters, cinds) == v[ind]\ntrue\n\nSee also: childindexshifted\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.childindexshifted-Tuple{ParallelUtilities.AbstractConstrainedProduct, Any}","page":"Reference","title":"ParallelUtilities.childindexshifted","text":"childindexshifted(ps::AbstractConstrainedProduct, ind)\n\nReturn a tuple containing the indices in the individual iterators given an index of ps.\n\nIf the iterators (r1, r2, ...) are used to generate ps, then return (i1, i2, ...) such that ps[ind] == (r1[i1], r2[i2], ...).\n\nExamples\n\njulia> iters = (1:5, 2:4, 1:3);\n\njulia> ps = ParallelUtilities.ProductSplit(iters, 7, 3);\n\njulia> psind = 4;\n\njulia> cinds = ParallelUtilities.childindexshifted(ps, psind)\n(3, 1, 2)\n\njulia> getindex.(iters, cinds) == ps[psind]\ntrue\n\nSee also: childindex\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.dropleading-Tuple{ParallelUtilities.AbstractConstrainedProduct{T, N, var\"#s1\"} where {T, N, var\"#s1\"<:Tuple{Vararg{AbstractUnitRange, N}}}}","page":"Reference","title":"ParallelUtilities.dropleading","text":"dropleading(ps::AbstractConstrainedProduct{T, N, NTuple{N,AbstractUnitRange}}) where {T,N}\n\nReturn a ProductSection leaving out the first iterator contained in ps. The range of values of the remaining iterators in the resulting ProductSection will be the same as in ps.\n\nExamples\n\njulia> ps = ParallelUtilities.ProductSplit((1:5, 2:4, 1:3), 7, 3);\n\njulia> collect(ps)\n7-element Vector{Tuple{Int64, Int64, Int64}}:\n (5, 4, 1)\n (1, 2, 2)\n (2, 2, 2)\n (3, 2, 2)\n (4, 2, 2)\n (5, 2, 2)\n (1, 3, 2)\n\njulia> ParallelUtilities.dropleading(ps) |> collect\n3-element Vector{Tuple{Int64, Int64}}:\n (4, 1)\n (2, 2)\n (3, 2)\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.elementwisemax!","page":"Reference","title":"ParallelUtilities.elementwisemax!","text":"elementwisemax!(x, y)\n\nBinary reduction operator that performs an elementwise max and stores the result inplace in x. The value of x is overwritten in the process.\n\nFunctionally elementwisemax!(x, y) is equivalent to x .= max.(x, y).\n\nnote: Note\nThe operator is assumed to be commutative.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelUtilities.elementwisemin!","page":"Reference","title":"ParallelUtilities.elementwisemin!","text":"elementwisemin!(x, y)\n\nBinary reduction operator that performs an elementwise min and stores the result inplace in x. The value of x is overwritten in the process.\n\nFunctionally elementwisemin!(x, y) is equivalent to x .= min.(x, y).\n\nnote: Note\nThe operator is assumed to be commutative.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelUtilities.elementwiseproduct!","page":"Reference","title":"ParallelUtilities.elementwiseproduct!","text":"elementwiseproduct!(x, y)\n\nBinary reduction operator that performs an elementwise product and stores the result inplace in x. The value of x is overwritten in the process.\n\nFunctionally elementwiseproduct!(x, y) is equivalent to x .= x .* y.\n\nnote: Note\nThe operator is assumed to be commutative.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelUtilities.elementwisesum!","page":"Reference","title":"ParallelUtilities.elementwisesum!","text":"elementwisesum!(x, y)\n\nBinary reduction operator that performs an elementwise product and stores the result inplace in x. The value of x is overwritten in the process.\n\nFunctionally elementwisesum!(x, y) is equivalent to x .= x .+ y.\n\nnote: Note\nThe operator is assumed to be commutative.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParallelUtilities.extrema_commonlastdim-Tuple{ParallelUtilities.AbstractConstrainedProduct{T, N, var\"#s1\"} where {T, N, var\"#s1\"<:Tuple{Vararg{AbstractUnitRange, N}}}}","page":"Reference","title":"ParallelUtilities.extrema_commonlastdim","text":"extrema_commonlastdim(ps::AbstractConstrainedProduct{T, N, <:NTuple{N,AbstractUnitRange}}) where {T,N}\n\nReturn the reverse - lexicographic extrema of values taken from ranges contained in ps, where the pairs of ranges are constructed by concatenating the ranges along each dimension with the last one.\n\nFor two ranges this simply returns ([first(ps)], [last(ps)]).\n\nExamples\n\njulia> ps = ParallelUtilities.ProductSplit((1:3, 4:7, 2:7), 10, 2);\n\njulia> collect(ps)\n8-element Vector{Tuple{Int64, Int64, Int64}}:\n (3, 6, 2)\n (1, 7, 2)\n (2, 7, 2)\n (3, 7, 2)\n (1, 4, 3)\n (2, 4, 3)\n (3, 4, 3)\n (1, 5, 3)\n\njulia> ParallelUtilities.extrema_commonlastdim(ps)\n([(1, 2), (6, 2)], [(3, 3), (5, 3)])\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.extremadims-Tuple{ParallelUtilities.AbstractConstrainedProduct}","page":"Reference","title":"ParallelUtilities.extremadims","text":"extremadims(ps::AbstractConstrainedProduct)\n\nCompute the extrema of the sections of all the ranges contained in ps. Functionally this is equivalent to\n\nmap(i -> extrema(ps, dims = i), 1:_niterators(ps))\n\nbut it is implemented more efficiently.\n\nReturns a Tuple containing the (min, max) pairs along each dimension, such that the i-th index of the result contains the extrema along the section of the i-th range contained locally.\n\nExamples\n\njulia> ps = ParallelUtilities.ProductSplit((1:2, 4:5), 2, 1);\n\njulia> collect(ps)\n2-element Vector{Tuple{Int64, Int64}}:\n (1, 4)\n (2, 4)\n\njulia> ParallelUtilities.extremadims(ps)\n((1, 2), (4, 4))\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.extremaelement-Tuple{ParallelUtilities.AbstractConstrainedProduct{T, N, var\"#s1\"} where {T, N, var\"#s1\"<:Tuple{Vararg{AbstractUnitRange, N}}}}","page":"Reference","title":"ParallelUtilities.extremaelement","text":"extremaelement(ps::AbstractConstrainedProduct; dims::Integer)\n\nCompute the extrema of the section of the range number dims contained in ps.\n\nExamples\n\njulia> ps = ParallelUtilities.ProductSplit((1:2, 4:5), 2, 1);\n\njulia> collect(ps)\n2-element Vector{Tuple{Int64, Int64}}:\n (1, 4)\n (2, 4)\n\njulia> ParallelUtilities.extremaelement(ps, dims = 1)\n(1, 2)\n\njulia> ParallelUtilities.extremaelement(ps, dims = 2)\n(4, 4)\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.indexinproduct-Union{Tuple{N}, Tuple{Tuple{Vararg{AbstractRange, N}}, Tuple{Vararg{Any, N}}}} where N","page":"Reference","title":"ParallelUtilities.indexinproduct","text":"indexinproduct(iterators::NTuple{N, AbstractRange}, val::NTuple{N, Any}) where {N}\n\nReturn the index of val in the outer product of iterators. Return nothing if val is not present.\n\nExamples\n\njulia> iterators = (1:4, 1:3, 3:5);\n\njulia> val = (2, 2, 4);\n\njulia> ind = ParallelUtilities.indexinproduct(iterators, val)\n18\n\njulia> collect(Iterators.product(iterators...))[ind] == val\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.localindex-Union{Tuple{T}, Tuple{ParallelUtilities.AbstractConstrainedProduct{T, N, Q} where {N, Q}, T}} where T","page":"Reference","title":"ParallelUtilities.localindex","text":"localindex(ps::AbstractConstrainedProduct{T}, val::T) where {T}\n\nReturn the index of val in ps. Return nothing if the value is not found.\n\nExamples\n\njulia> ps = ParallelUtilities.ProductSplit((1:3, 4:5:20), 3, 2);\n\njulia> collect(ps)\n4-element Vector{Tuple{Int64, Int64}}:\n (2, 9)\n (3, 9)\n (1, 14)\n (2, 14)\n\njulia> ParallelUtilities.localindex(ps, (3, 9))\n2\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.masternodeindex-Tuple{ParallelUtilities.SegmentedOrderedBinaryTree, Any}","page":"Reference","title":"ParallelUtilities.masternodeindex","text":"masternodeindex(tree::SegmentedOrderedBinaryTree, p)\n\nGiven the top worker p on one node, compute the serial order of the host that it corresponds to.\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.maximumelement-Tuple{ParallelUtilities.AbstractConstrainedProduct{T, N, var\"#s1\"} where {T, N, var\"#s1\"<:Tuple{Vararg{AbstractUnitRange, N}}}}","page":"Reference","title":"ParallelUtilities.maximumelement","text":"maximumelement(ps::AbstractConstrainedProduct; dims::Integer)\n\nCompute the maximum value of the section of the range number dims contained in ps.\n\nExamples\n\njulia> ps = ParallelUtilities.ProductSplit((1:2, 4:5), 2, 1);\n\njulia> collect(ps)\n2-element Vector{Tuple{Int64, Int64}}:\n (1, 4)\n (2, 4)\n\njulia> ParallelUtilities.maximumelement(ps, dims = 1)\n2\n\njulia> ParallelUtilities.maximumelement(ps, dims = 2)\n4\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.minimumelement-Tuple{ParallelUtilities.AbstractConstrainedProduct{T, N, var\"#s1\"} where {T, N, var\"#s1\"<:Tuple{Vararg{AbstractUnitRange, N}}}}","page":"Reference","title":"ParallelUtilities.minimumelement","text":"minimumelement(ps::AbstractConstrainedProduct; dims::Integer)\n\nCompute the minimum value of the section of the range number dims contained in ps.\n\nExamples\n\njulia> ps = ParallelUtilities.ProductSplit((1:2, 4:5), 2, 1);\n\njulia> collect(ps)\n2-element Vector{Tuple{Int64, Int64}}:\n (1, 4)\n (2, 4)\n\njulia> ParallelUtilities.minimumelement(ps, dims = 1)\n1\n\njulia> ParallelUtilities.minimumelement(ps, dims = 2)\n4\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.nelements-Tuple{ParallelUtilities.AbstractConstrainedProduct{T, N, var\"#s1\"} where {T, N, var\"#s1\"<:Tuple{Vararg{AbstractUnitRange, N}}}}","page":"Reference","title":"ParallelUtilities.nelements","text":"nelements(ps::AbstractConstrainedProduct{T, N, <:NTuple{N,AbstractUnitRange}}; dims::Integer) where {T,N}\n\nCompute the number of unique values in the section of the dims-th range contained in ps.\n\nThe function is defined currently only for iterator products of AbstractUnitRanges.\n\nExamples\n\njulia> ps = ParallelUtilities.ProductSplit((1:5, 2:4, 1:3), 7, 3);\n\njulia> collect(ps)\n7-element Vector{Tuple{Int64, Int64, Int64}}:\n (5, 4, 1)\n (1, 2, 2)\n (2, 2, 2)\n (3, 2, 2)\n (4, 2, 2)\n (5, 2, 2)\n (1, 3, 2)\n\njulia> ParallelUtilities.nelements(ps, dims = 1)\n5\n\njulia> ParallelUtilities.nelements(ps, dims = 2)\n3\n\njulia> ParallelUtilities.nelements(ps, dims = 3)\n2\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.pmapbatch-Tuple{Any, Distributed.AbstractWorkerPool, Vararg{Any, N} where N}","page":"Reference","title":"ParallelUtilities.pmapbatch","text":"pmapbatch(f, [pool::AbstractWorkerPool], iterators...)\n\nCarry out a pmap with the iterators divided evenly among the available workers.\n\nSee also: pmapreduce\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.pmapbatch_productsplit-Tuple{Any, Distributed.AbstractWorkerPool, Vararg{Any, N} where N}","page":"Reference","title":"ParallelUtilities.pmapbatch_productsplit","text":"pmapbatch_productsplit(f, [pool::AbstractWorkerPool], iterators...)\n\nCarry out a pmap with the outer product of iterators divided evenly among the available workers. The function f must accept a collection of Tuples.\n\nSee also: pmapbatch, pmapreduce_productsplit\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.pmapreduce-Tuple{Any, Any, Distributed.AbstractWorkerPool, Vararg{Any, N} where N}","page":"Reference","title":"ParallelUtilities.pmapreduce","text":"pmapreduce(f, op, [pool::AbstractWorkerPool], iterators...; reducekw...)\n\nEvaluate a parallel mapreduce over the elements from iterators. For multiple iterators, apply f elementwise.\n\nThe keyword arguments reducekw are passed on to the reduction.\n\nSee also: pmapreduce_productsplit\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.pmapreduce_productsplit-Tuple{Any, Any, Distributed.AbstractWorkerPool, Vararg{Any, N} where N}","page":"Reference","title":"ParallelUtilities.pmapreduce_productsplit","text":"pmapreduce_productsplit(f, op, [pool::AbstractWorkerPool], iterators...; reducekw...)\n\nEvaluate a parallel mapreduce over the outer product of elements from iterators. The product of iterators is split over the workers available, and each worker is assigned a section of the product. The function f should accept a single argument that is a collection of Tuples.\n\nThe keyword arguments reducekw are passed on to the reduction.\n\nSee also: pmapreduce\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.procrange_recast-Tuple{ParallelUtilities.AbstractConstrainedProduct, Integer}","page":"Reference","title":"ParallelUtilities.procrange_recast","text":"procrange_recast(ps::AbstractConstrainedProduct, np_new::Integer)\n\nReturn the range of processor ranks that would contain the values in ps if the iterators used to construct ps were split across np_new processes.\n\nExamples\n\njulia> iters = (1:10, 4:6, 1:4);\n\njulia> ps = ParallelUtilities.ProductSplit(iters, 5, 2); # split across 5 processes initially\n\njulia> ParallelUtilities.procrange_recast(ps, 10) # If `iters` were spread across 10 processes\n3:4\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.procrange_recast-Tuple{Tuple{AbstractRange, Vararg{AbstractRange, N} where N}, ParallelUtilities.AbstractConstrainedProduct, Integer}","page":"Reference","title":"ParallelUtilities.procrange_recast","text":"procrange_recast(iterators::Tuple{Vararg{AbstractRange}}, ps, np_new::Integer)\n\nReturn the range of processor ranks that would contain the values in ps if the outer produce of the ranges in iterators is split across np_new workers.\n\nThe values contained in ps should be a subsection of the outer product of the ranges in iterators.\n\nExamples\n\njulia> iters = (1:10, 4:6, 1:4);\n\njulia> ps = ParallelUtilities.ProductSplit(iters, 5, 2);\n\njulia> ParallelUtilities.procrange_recast(iters, ps, 10)\n3:4\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.whichproc-Tuple{Tuple{AbstractRange, Vararg{AbstractRange, N} where N}, Any, Integer}","page":"Reference","title":"ParallelUtilities.whichproc","text":"whichproc(iterators::Tuple{Vararg{AbstractRange}}, val::Tuple, np::Integer)\n\nReturn the processor rank that will contain val if the outer product of the ranges contained in iterators is split evenly across np processors.\n\nExamples\n\njulia> iters = (1:4, 2:3);\n\njulia> np = 2;\n\njulia> ParallelUtilities.ProductSplit(iters, np, 2) |> collect\n4-element Vector{Tuple{Int64, Int64}}:\n (1, 3)\n (2, 3)\n (3, 3)\n (4, 3)\n\njulia> ParallelUtilities.whichproc(iters, (2, 3), np)\n2\n\n\n\n\n\n","category":"method"},{"location":"api/#ParallelUtilities.whichproc_localindex-Tuple{Tuple{Vararg{AbstractRange, N} where N}, Tuple, Integer}","page":"Reference","title":"ParallelUtilities.whichproc_localindex","text":"whichproc_localindex(iterators::Tuple{Vararg{AbstractRange}}, val::Tuple, np::Integer)\n\nReturn (rank, ind), where rank is the rank of the worker that val will reside on if the outer product of the ranges in iterators is spread over np workers, and ind is the index of val in the local section on that worker.\n\nExamples\n\njulia> iters = (1:4, 2:8);\n\njulia> np = 10;\n\njulia> ParallelUtilities.whichproc_localindex(iters, (2, 4), np)\n(4, 1)\n\njulia> ParallelUtilities.ProductSplit(iters, np, 4) |> collect\n3-element Vector{Tuple{Int64, Int64}}:\n (2, 4)\n (3, 4)\n (4, 4)\n\n\n\n\n\n","category":"method"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"DocTestSetup  = quote\n    using ParallelUtilities\nend","category":"page"},{"location":"pmapreduce/#Parallel-mapreduce","page":"Mapreduce","title":"Parallel mapreduce","text":"","category":"section"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"There are two modes of evaluating a parallel mapreduce that vary only in the arguments that the mapping function accepts.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"Iterated zip, where one element from the zipped iterators is splatted and passed as arguments to the mapping function. In this case the function must accept as many arguments as the number of iterators passed to mapreduce. This is analogous to a serial mapreduce\nNon-iterated product, in which case the iterator product of the arguments is distributed evenly across the workers. The mapping function in this case should accept one argument that is a collection of Tuples of values. It may iterate over the argument to obtain the individual Tuples.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"Each process involved in a pmapreduce operation carries out a local mapreduce, followed by a reduction across processes. The reduction is carried out in the form of a binary tree. The reduction happens in three stages:","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"A local reduction as a part of mapreduce\nA reduction on the host across the workers on the same host. Typically on an HPC system there is an independent reduction on each node across the processes on that node.\nA global reduction across hosts.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The reduction operator is assumed to be associative, and reproducibility of floating-point operations is not guaranteed. For associative reductions look into various mapfold* methods provided by other packages, such as Transducers. The reduction operator is not assumed to be commutative.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"A pmapreduce might only benefit in performance if the mapping function runs for longer than the communication overhead across processes, or if each process has dedicated memory and returns large arrays that may not be collectively stored on one process.","category":"page"},{"location":"pmapreduce/#Iterated-Zip","page":"Mapreduce","title":"Iterated Zip","text":"","category":"section"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The syntax for a parallel map-reduce operation is quite similar to the serial mapreduce, with the replacement of mapreduce by pmapreduce.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"Serial:","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> mapreduce(x -> x^2, +, 1:100_000)\n333338333350000","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"Parallel:","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> pmapreduce(x -> x^2, +, 1:100_000)\n333338333350000","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"We may check that parallel evaluation helps in performance for a long-running process.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> nworkers()\n2\n\njulia> @time mapreduce(x -> (sleep(1); x^2), +, 1:6);\n  6.079191 seconds (54.18 k allocations: 3.237 MiB, 1.10% compilation time)\n\njulia> @time pmapreduce(x -> (sleep(1); x^2), +, 1:6);\n  3.365979 seconds (91.57 k allocations: 5.473 MiB, 0.87% compilation time)","category":"page"},{"location":"pmapreduce/#Non-iterated-product","page":"Mapreduce","title":"Non-iterated product","text":"","category":"section"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The second mode of usage is similar to MPI, where each process evaluates the same function once for different arguments. This is called using","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"pmapreduce_productsplit(f, op, iterators...)","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"In this function, the iterator product of the argument iterators is split evenly across the workers, and the function f on each process receives one such section according to its rank. The argument is an iterator similar to an iterator product, and looping over it would produce Tuples (iterators[1][i], iterators[2][i], ...) where the index i depends on the rank of the worker as well as the local loop index.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"As an example, we run this with 2 workers:","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> pmapreduce_productsplit(ps -> (@show collect(ps)), vcat, 1:4)\n      From worker 2:    collect(ps) = [(1,), (2,)]\n      From worker 3:    collect(ps) = [(3,), (4,)]\n4-element Vector{Tuple{Int64}}:\n (1,)\n (2,)\n (3,)\n (4,)\n\njulia> pmapreduce_productsplit(ps -> (@show collect(ps)), vcat, 1:3, 1:2)\n      From worker 2:    collect(ps) = [(1, 1), (2, 1), (3, 1)]\n      From worker 3:    collect(ps) = [(1, 2), (2, 2), (3, 2)]\n6-element Vector{Tuple{Int64, Int64}}:\n (1, 1)\n (2, 1)\n (3, 1)\n (1, 2)\n (2, 2)\n (3, 2)","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"Note that in each case the mapping function receives the entire collection of arguments in one go, unlike a standard mapreduce where the function receives the arguments individually. This is chosen so that the function may perform any one-time compute-intensive task for the entire range before looping over the argument values.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"Each process might return one or more values that are subsequently reduced in parallel.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"note: Note\nAt present the iterators passed as arguments to pmapreduce_productsplit may only be strictly increasing ranges. This might be relaxed in the future.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The argument ps passed on to each worker is a ParallelUtilities.ProductSplit object. This has several methods defined for it that might aid in evaluating the mapping function locally.","category":"page"},{"location":"pmapreduce/#ProductSplit","page":"Mapreduce","title":"ProductSplit","text":"","category":"section"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"A ProductSplit object ps holds the section of the iterator product that is assigned to the worker. It also encloses the worker rank and the size of the worker pool, similar to MPI's Comm_rank and Comm_size. These may be accessed as workerrank(ps) and nworkers(ps). Unlike MPI though, the rank goes from 1 to np. An example where the worker rank is used (on 2 workers) is","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> pmapreduce_productsplit(ps -> ones(2) * workerrank(ps), hcat, 1:nworkers())\n2×2 Matrix{Float64}:\n 1.0  2.0\n 1.0  2.0","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The way to construct a ProductSplit object is ParallelUtilities.ProductSplit(tuple_of_iterators, nworkers, worker_rank)","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> ps = ParallelUtilities.ProductSplit((1:2, 3:4), 2, 1)\n2-element ProductSplit [(1, 3), ... , (2, 3)]\n\njulia> ps |> collect\n2-element Vector{Tuple{Int64, Int64}}:\n (1, 3)\n (2, 3)","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"A ProductSplit that wraps AbstractUnitRanges has several efficient functions defined for it, such as length, minimumelement, maximumelement and getindex, each of which returns in O(1) without iterating over the object.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> ps[1]\n(1, 3)","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The function maximumelement, minimumelement and extremaelement treat the ProductSplit object as a linear view of an n-dimensional iterator product. These functions look through the elements in the dim-th dimension of the iterator product, and if possible, return the corresponding extremal element in O(1) time. Similarly, for a ProductSplit object that wraps AbstractUnitRanges, it's possible to know if a value is contained in the iterator in O(1) time.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> ps = ParallelUtilities.ProductSplit((1:100_000, 1:100_000, 1:100_000), 25000, 1500)\n40000000000-element ProductSplit [(1, 1, 5997), ... , (100000, 100000, 6000)]\n\njulia> @btime (3,3,5998) in $ps\n  111.399 ns (0 allocations: 0 bytes)\ntrue\n\njulia> @btime ParallelUtilities.maximumelement($ps, dims = 1)\n  76.534 ns (0 allocations: 0 bytes)\n100000\n\njulia> @btime ParallelUtilities.minimumelement($ps, dims = 2)\n  73.724 ns (0 allocations: 0 bytes)\n1\n\njulia> @btime ParallelUtilities.extremaelement($ps, dims = 2)\n  76.332 ns (0 allocations: 0 bytes)\n(1, 100000)","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The number of unique elements along a particular dimension may be obtained as","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> @btime ParallelUtilities.nelements($ps, dims = 3)\n  118.441 ns (0 allocations: 0 bytes)\n4","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"It's also possible to drop the leading dimension of a ProductSplit that wraps AbstractUnitRanges to obtain an analogous operator that contains the unique elements along the remaining dimension. This is achieved using ParallelUtilities.dropleading.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> ps = ParallelUtilities.ProductSplit((1:3, 1:3, 1:2), 4, 2)\n5-element ProductSplit [(3, 2, 1), ... , (1, 1, 2)]\n\njulia> collect(ps)\n5-element Vector{Tuple{Int64, Int64, Int64}}:\n (3, 2, 1)\n (1, 3, 1)\n (2, 3, 1)\n (3, 3, 1)\n (1, 1, 2)\n\njulia> ps2 = ParallelUtilities.dropleading(ps)\n3-element ProductSection [(2, 1), ... , (1, 2)]\n\njulia> collect(ps2)\n3-element Vector{Tuple{Int64, Int64}}:\n (2, 1)\n (3, 1)\n (1, 2)","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The process may be repeated multiple times:","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> collect(ParallelUtilities.dropleading(ps2))\n2-element Vector{Tuple{Int64}}:\n (1,)\n (2,)","category":"page"},{"location":"pmapreduce/#Reduction-Operators","page":"Mapreduce","title":"Reduction Operators","text":"","category":"section"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"Any standard Julia reduction operator may be passed to pmapreduce. Aside from this, this package defines certain operators that may be used as well in a reduction.","category":"page"},{"location":"pmapreduce/#Broadcasted-elementwise-operators","page":"Mapreduce","title":"Broadcasted elementwise operators","text":"","category":"section"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The general way to construct an elementwise operator using this package is using ParallelUtilities.BroadcastFunction.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"For example, a broadcasted sum operator may be constructed using","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> ParallelUtilities.BroadcastFunction(+);","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The function call ParallelUtilities.BroadcastFunction(op)(x, y) perform the fused elementwise operation op.(x, y).","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"note: Julia 1.6 and above\nJulia versions above v\"1.6\" provide a function Base.BroadcastFunction which is equivalent to ParallelUtilities.BroadcastFunction.","category":"page"},{"location":"pmapreduce/#Inplace-assignment","page":"Mapreduce","title":"Inplace assignment","text":"","category":"section"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The function ParallelUtilities.broadcastinplace may be used to construct a binary operator that broadcasts a function over its arguments and stores the result inplace in one of the arguments. This is particularly useful if the results in intermediate evaluations are not important, as this cuts down on allocations in the reduction.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"Several operators for common functions are pre-defined for convenience.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"ParallelUtilities.elementwisesum!\nParallelUtilities.elementwiseproduct!\nParallelUtilities.elementwisemin!\nParallelUtilities.elementwisemax!","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"Each of these functions overwrites the first argument with the result.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"warn: Warn\nThe pre-defined elementwise operators are assumed to be commutative, so, if used in pmapreduce, the order of arguments passed to the function is not guaranteed. In particular this might not be in order of the workerrank. These functions should only be used if both the arguments support the inplace assignment, eg. if they have identical axes.","category":"page"},{"location":"pmapreduce/#Flip","page":"Mapreduce","title":"Flip","text":"","category":"section"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"The ParallelUtilities.Flip function may be used to wrap a binary function to flips the order of arguments. For example","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> vcat(1,2)\n2-element Vector{Int64}:\n 1\n 2\n\njulia> ParallelUtilities.Flip(vcat)(1,2)\n2-element Vector{Int64}:\n 2\n 1","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"Flip may be combined with inplace assignment operators to change the argument that is overwritten.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> x = ones(3); y = ones(3);\n\njulia> op1 = ParallelUtilities.elementwisesum!; # overwrites the first argument\n\njulia> op1(x, y); x\n3-element Vector{Float64}:\n 2.0\n 2.0\n 2.0\n\njulia> x = ones(3); y = ones(3);\n\njulia> op2 = ParallelUtilities.Flip(op1); # ovrewrites the second argument\n\njulia> op2(x, y); y\n3-element Vector{Float64}:\n 2.0\n 2.0\n 2.0","category":"page"},{"location":"pmapreduce/#BroadcastStack","page":"Mapreduce","title":"BroadcastStack","text":"","category":"section"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"This function may be used to combine arrays having overlapping axes to obtain a new array that spans the union of axes of the arguments. The overlapping section is computed by applying the reduction function to that section.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"We construct a function that concatenates arrays along the first dimension with overlapping indices summed.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> f = ParallelUtilities.BroadcastStack(+, 1);","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"We apply this to two arrays having different indices","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"julia> f(ones(2), ones(4))\n4-element Vector{Float64}:\n 2.0\n 2.0\n 1.0\n 1.0","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"This function is useful to reduce OffsetArrays where each process evaluates a potentially overlapping section of the entire array.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"note: Note\nA BroadcastStack function requires its arguments to have the same dimensionality, and identical axes along non-concatenated dimensions. In particular it is not possible to block-concatenate arrays using this function.","category":"page"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"note: Note\nA BroadcastStack function does not operate in-place.","category":"page"},{"location":"pmapreduce/#Commutative","page":"Mapreduce","title":"Commutative","text":"","category":"section"},{"location":"pmapreduce/","page":"Mapreduce","title":"Mapreduce","text":"In general this package does not assume that a reduction operator is commutative. It's possible to declare an operator to be commutative in its arguments by wrapping it in the tag ParallelUtilities.Commutative. ","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"DocTestSetup  = quote\n    using ParallelUtilities\nend","category":"page"},{"location":"#ParallelUtilities.jl","page":"ParallelUtilities","title":"ParallelUtilities.jl","text":"","category":"section"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"The ParallelUtilities module defines certain functions that are useful in a parallel mapreduce operation, with particular focus on HPC systems. The approach is similar to a @distributed (op) for loop, where the entire section of iterators is split evenly across workers and reduced locally, followed by a global reduction. The operation is not load-balanced at present, and does not support retry on error.","category":"page"},{"location":"#Performance","page":"ParallelUtilities","title":"Performance","text":"","category":"section"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"The pmapreduce-related functions are expected to be more performant than @distributed for loops. As an example, running the following on a Slurm cluster using 2 nodes with 28 cores on each leads to","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"julia> using Distributed\n\njulia> using ParallelUtilities\n\njulia> @everywhere f(x) = ones(10_000, 1_000);\n\njulia> A = @time @distributed (+) for i=1:nworkers()\n                f(i)\n            end;\n 22.637764 seconds (3.35 M allocations: 8.383 GiB, 16.50% gc time, 0.09% compilation time)\n\njulia> B = @time pmapreduce(f, +, 1:nworkers());\n  2.170926 seconds (20.47 k allocations: 77.117 MiB)\n\njulia> A == B\ntrue","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"The difference increases with the size of data as well as the number of workers. This is because the pmapreduce* functions defined in this package perform local reductions before communicating data across nodes. Note that in this case the same operation may be carried out elementwise to obtain better performance.","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"julia> @everywhere elsum(x,y) = x .+= y;\n\njulia> A = @time @distributed (elsum) for i=1:nworkers()\n               f(i)\n           end;\n 20.537353 seconds (4.74 M allocations: 4.688 GiB, 2.56% gc time, 1.26% compilation time)\n\njulia> B = @time pmapreduce(f, elsum, 1:nworkers());\n  1.791662 seconds (20.50 k allocations: 77.134 MiB)","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"A similar evaluation on 560 cores (20 nodes) takes","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"julia> @time for i = 1:10; pmapreduce(f, +, 1:nworkers()); end\n145.963834 seconds (2.53 M allocations: 856.693 MiB, 0.12% gc time)\n\njulia> @time for i = 1:10; pmapreduce(f, elsum, 1:nworkers()); end\n133.810309 seconds (2.53 M allocations: 856.843 MiB, 0.13% gc time)","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"An example of a mapreduce operation involving large arrays (comparable to the memory allocated to each core) evaluated on 56 cores is","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"julia> @everywhere f(x) = ones(12_000, 20_000);\n\njulia> @time ParallelUtilities.pmapreduce(f, elsum, 1:nworkers());\n 36.824788 seconds (26.40 k allocations: 1.789 GiB, 0.05% gc time)","category":"page"},{"location":"#Comparison-with-other-parallel-mapreduce-packages","page":"ParallelUtilities","title":"Comparison with other parallel mapreduce packages","text":"","category":"section"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"Other packages that perform parallel mapreduce are ParallelMapReduce and Transducers. The latter provides a foldxd function that performs an associative distributed mapfold. The performances of these functions compared to this package (measured on 1 node with 28 cores) are listed below:","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"julia> @everywhere f(x) = ones(10_000, 10_000);\n\njulia> A = @time ParallelUtilities.pmapreduce(f, +, 1:nworkers());\n 10.105696 seconds (14.03 k allocations: 763.511 MiB)\n\njulia> B = @time ParallelMapReduce.pmapreduce(f, +, 1:nworkers(), algorithm = :reduction_local);\n 30.955381 seconds (231.93 k allocations: 41.200 GiB, 7.63% gc time, 0.23% compilation time)\n\njulia> C = @time Transducers.foldxd(+, 1:nworkers() |> Transducers.Map(f));\n 30.154166 seconds (655.40 k allocations: 41.015 GiB, 8.65% gc time, 1.03% compilation time)\n\njulia> A == B == C\ntrue","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"Note that at present the performances of the pmapreduce* functions defined in this package are not comparable to equivalent MPI implementations. For example, an MPI mapreduce operation using MPIMapReduce.jl computes an inplace sum over 10_000 x 10_000 matrices on each core in","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"3.413968 seconds (3.14 M allocations: 1.675 GiB, 2.99% gc time)","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"whereas this package computes it in","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"julia> @time ParallelUtilities.pmapreduce(f, elsum, 1:nworkers());\n  7.264023 seconds (12.46 k allocations: 763.450 MiB, 1.69% gc time)","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"This performance gap might reduce in the future.","category":"page"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"note: Note\nThe timings have all been measured on Julia 1.6 on an HPC cluster that has nodes with with 2 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz CPUs (\"Broadwell\", 14 cores/socket, 28 cores/node). They are also measured for subsequent runs after an initial precompilation step. The exact evaluation time might also vary depending on the cluster load.","category":"page"},{"location":"#Known-issues","page":"ParallelUtilities","title":"Known issues","text":"","category":"section"},{"location":"","page":"ParallelUtilities","title":"ParallelUtilities","text":"This package currently does not implement a specialized mapreduce for arrays, so the behavior might differ for specialized array argument types (eg. DistributedArrays). This might change in the future.\nThis package deals with distributed (multi-core) parallelism, and at this moment it has not been tested alongside multi-threading.","category":"page"},{"location":"clusterquery/","page":"ClusterQueryUtils","title":"ClusterQueryUtils","text":"DocTestSetup  = quote\n    using ParallelUtilities\n    using ParallelUtilities.ClusterQueryUtils\nend","category":"page"},{"location":"clusterquery/#Cluster-Query-Utilities","page":"ClusterQueryUtils","title":"Cluster Query Utilities","text":"","category":"section"},{"location":"clusterquery/","page":"ClusterQueryUtils","title":"ClusterQueryUtils","text":"These are a collection of helper functions that are used in ParallelUtilities, but may be used independently as well to obtain information about the cluster on which codes are being run.","category":"page"},{"location":"clusterquery/","page":"ClusterQueryUtils","title":"ClusterQueryUtils","text":"To use these functions run","category":"page"},{"location":"clusterquery/","page":"ClusterQueryUtils","title":"ClusterQueryUtils","text":"julia> using ParallelUtilities.ClusterQueryUtils","category":"page"},{"location":"clusterquery/","page":"ClusterQueryUtils","title":"ClusterQueryUtils","text":"The functions defined in this module are:","category":"page"},{"location":"clusterquery/","page":"ClusterQueryUtils","title":"ClusterQueryUtils","text":"Modules = [ParallelUtilities.ClusterQueryUtils]","category":"page"},{"location":"clusterquery/#ParallelUtilities.ClusterQueryUtils.hostnames","page":"ClusterQueryUtils","title":"ParallelUtilities.ClusterQueryUtils.hostnames","text":"hostnames([procs = workers()])\n\nReturn the hostname of each worker in procs. This is obtained by evaluating Libc.gethostname() on each worker asynchronously.\n\n\n\n\n\n","category":"function"},{"location":"clusterquery/#ParallelUtilities.ClusterQueryUtils.nodenames","page":"ClusterQueryUtils","title":"ParallelUtilities.ClusterQueryUtils.nodenames","text":"nodenames([procs = workers()])\n\nReturn the unique hostnames that the workers in procs lie on. On an HPC system these are usually the hostnames of the nodes involved.\n\n\n\n\n\n","category":"function"},{"location":"clusterquery/#ParallelUtilities.ClusterQueryUtils.nprocs_node","page":"ClusterQueryUtils","title":"ParallelUtilities.ClusterQueryUtils.nprocs_node","text":"nprocs_node([procs = workers()])\n\nReturn the number of workers on each host. On an HPC system this would return the number of workers on each node.\n\n\n\n\n\n","category":"function"},{"location":"clusterquery/#ParallelUtilities.ClusterQueryUtils.procs_node","page":"ClusterQueryUtils","title":"ParallelUtilities.ClusterQueryUtils.procs_node","text":"procs_node([procs = workers()])\n\nReturn the worker ids on each host of the cluster. On an HPC system this would return the workers on each node.\n\n\n\n\n\n","category":"function"}]
}
